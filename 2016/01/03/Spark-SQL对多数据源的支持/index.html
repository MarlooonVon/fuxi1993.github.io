<!doctype html>
<html class="theme-next use-motion theme-next-mist">
<head>
  

<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />








  <link rel="stylesheet" type="text/css" href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5"/>



  
    <link href='//fonts.lug.ustc.edu.cn/css?family=Lato:300,400,700,400italic&subset=latin,latin-ext' rel='stylesheet' type='text/css'>
  


<link rel="stylesheet" type="text/css" href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" />

<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.5.2"/>




  <meta name="keywords" content="SQL,Spark," />





  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=0.4.5.2" />


<meta name="description" content="1. 什么是Spark SQL?Spark SQL是一个针对结构化数据的Spark框架，它提供了一个叫DataFrames的编程抽象，而且还可以扮演分布式SQL查询引擎的角色。
1.1 Spark SQL提供的数据抽象:DataFrame一个DataFrame是一个列式数据构成的数据集合。它概念上等同于关系数据库中的一张表或者Python/R语言中的框架，但是在底层有更多的优化。DataFrame">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark SQL对多数据源的支持">
<meta property="og:url" content="http://yoursite.com/2016/01/03/Spark-SQL对多数据源的支持/index.html">
<meta property="og:site_name" content="My Blog">
<meta property="og:description" content="1. 什么是Spark SQL?Spark SQL是一个针对结构化数据的Spark框架，它提供了一个叫DataFrames的编程抽象，而且还可以扮演分布式SQL查询引擎的角色。
1.1 Spark SQL提供的数据抽象:DataFrame一个DataFrame是一个列式数据构成的数据集合。它概念上等同于关系数据库中的一张表或者Python/R语言中的框架，但是在底层有更多的优化。DataFrame">
<meta property="og:updated_time" content="2016-01-03T15:38:49.074Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark SQL对多数据源的支持">
<meta name="twitter:description" content="1. 什么是Spark SQL?Spark SQL是一个针对结构化数据的Spark框架，它提供了一个叫DataFrames的编程抽象，而且还可以扮演分布式SQL查询引擎的角色。
1.1 Spark SQL提供的数据抽象:DataFrame一个DataFrame是一个列式数据构成的数据集合。它概念上等同于关系数据库中的一张表或者Python/R语言中的框架，但是在底层有更多的优化。DataFrame">


<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: 'Mist',
    sidebar: 'post'
  };
</script>



  <title> Spark SQL对多数据源的支持 | My Blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  






  <div class="container one-column page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">My Blog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu ">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-/"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-/categories"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-/archives"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-/tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content"> 

  <div id="posts" class="posts-expand">
    

  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Spark SQL对多数据源的支持
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            发表于
            <time itemprop="dateCreated" datetime="2016-01-03T19:08:17+08:00" content="2016-01-03">
              2016-01-03
            </time>
          </span>

          

          
            
          

          

        </div>
      </header>
    


    <div class="post-body">

      
      

      
        <span itemprop="articleBody"><h2 id="1-_什么是Spark_SQL?">1. 什么是Spark SQL?</h2><p>Spark SQL是一个针对结构化数据的Spark框架，它提供了一个叫DataFrames的编程抽象，而且还可以扮演分布式SQL查询引擎的角色。</p>
<h3 id="1-1_Spark_SQL提供的数据抽象:DataFrame">1.1 Spark SQL提供的数据抽象:DataFrame</h3><p>一个DataFrame是一个列式数据构成的数据集合。它概念上等同于关系数据库中的一张表或者Python/R语言中的框架，但是在底层有更多的优化。DataFrame可以通过广泛的数据源来构建，例如：结构化的数据文件、Hive中的表、外部的数据库，或者已经存在的RDD。</p>
<blockquote>
<p>关于Spark SQL更多的介绍，可见<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#creating-dataframes" target="_blank" rel="external">Spark SQL and DataFrame Guide</a></p>
</blockquote>
<h2 id="2-_支持JSON数据类型">2. 支持JSON数据类型</h2><p>以下关于Spark SQL对JSON文件的处理过程同样适用与其他各种类型的外部数据源，DataFrame数据结构提供了统一的数据抽象。</p>
<h3 id="2-1_开始：SQLContext">2.1 开始：SQLContext</h3><p>Spark SQL的入口是SQLContext类（或者是它的子类）。为了创建SQLContext，只需要SparkContext。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">sc</span>:</span> <span class="type">SparkContext</span> <span class="comment">// An existing SparkContext.</span></span><br><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">sqlContext</span> =</span> <span class="keyword">new</span> org.apache.spark.sql.<span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br></pre></td></tr></table></figure></p>
<h3 id="2-2_创建DataFrame">2.2 创建DataFrame</h3><p>利用SQLContext，应用可以从一个已存在的RDD（需要知道Schema）、Hive Table、各种外部数据源来创建DataFrame。<br>下面是基于一个JSON文件的内容创建一个DataFrame：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">sc</span>:</span> <span class="type">SparkContext</span> <span class="comment">// An existing SparkContext.</span></span><br><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">sqlContext</span> =</span> <span class="keyword">new</span> org.apache.spark.sql.<span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">df</span> =</span> sqlContext.read.json(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Displays the content of the DataFrame to stdout</span></span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure></p>
<h3 id="2-3_DataFrame的操作">2.3 DataFrame的操作</h3><p>DataFrame以Scala, Java, Python语言为结构化的数据操作提供了域限定性语言。<br>下面展示了使用DataFrame进行一些基本的结构化数据操作：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">sc</span>:</span> <span class="type">SparkContext</span> <span class="comment">// An existing SparkContext.</span></span><br><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">sqlContext</span> =</span> <span class="keyword">new</span> org.apache.spark.sql.<span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create the DataFrame</span></span><br><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">df</span> =</span> sqlContext.read.json(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Show the content of the DataFrame</span></span><br><span class="line">df.show()</span><br><span class="line"><span class="comment">// age  name</span></span><br><span class="line"><span class="comment">// null Michael</span></span><br><span class="line"><span class="comment">// 30   Andy</span></span><br><span class="line"><span class="comment">// 19   Justin</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Print the schema in a tree format</span></span><br><span class="line">df.printSchema()</span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">// |-- age: long (nullable = true)</span></span><br><span class="line"><span class="comment">// |-- name: string (nullable = true)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Select only the "name" column</span></span><br><span class="line">df.select(<span class="string">"name"</span>).show()</span><br><span class="line"><span class="comment">// name</span></span><br><span class="line"><span class="comment">// Michael</span></span><br><span class="line"><span class="comment">// Andy</span></span><br><span class="line"><span class="comment">// Justin</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Select everybody, but increment the age by 1</span></span><br><span class="line">df.select(df(<span class="string">"name"</span>), df(<span class="string">"age"</span>) + <span class="number">1</span>).show()</span><br><span class="line"><span class="comment">// name    (age + 1)</span></span><br><span class="line"><span class="comment">// Michael null</span></span><br><span class="line"><span class="comment">// Andy    31</span></span><br><span class="line"><span class="comment">// Justin  20</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Select people older than 21</span></span><br><span class="line">df.filter(df(<span class="string">"age"</span>) &gt; <span class="number">21</span>).show()</span><br><span class="line"><span class="comment">// age name</span></span><br><span class="line"><span class="comment">// 30  Andy</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Count people by age</span></span><br><span class="line">df.groupBy(<span class="string">"age"</span>).count().show()</span><br><span class="line"><span class="comment">// age  count</span></span><br><span class="line"><span class="comment">// null 1</span></span><br><span class="line"><span class="comment">// 19   1</span></span><br><span class="line"><span class="comment">// 30   1</span></span><br></pre></td></tr></table></figure></p>
<p>关于DataFrame的全部的操作列表，可见<a href="http://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrame.html" target="_blank" rel="external">the API Documentation</a>。<br>除了上面这些简单的列操作已经表达式操作，DataFrame还有一个丰富的库来支持字符串操作，数值运算等。完整的列表可见<a href="http://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/functions.html" target="_blank" rel="external">DataFrame Function Reference</a>。</p>
<h3 id="2-4_运行SQL查询语句">2.4 运行SQL查询语句</h3><p>SQLContext类中的sql函数可以使应用程序化的运行SQL 查询语句，并且以DataFrame的形式返回。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">sqlContext</span> =</span> ...  <span class="comment">// An existing SQLContext</span></span><br><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">df</span> =</span> sqlContext.sql(<span class="string">"SELECT * FROM table"</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="3-_解释RDD">3. 解释RDD</h2><p>Spark SQL支持两种不同的方法将现有的RDD转化为DataFrames。第一种方法是使用反射来导出一个RDD的schema。写spark的应用程序时，在已知数据的schema的时候，反射的方法可以帮助写出更简介的代码。<br>第二种方法，通过一个编程接口（这个编程接口可以帮助你构建一个schema并且应用到一个已经存在的RDD上）。</p>
<h3 id="3-1_利用反射来导出Schema">3.1 利用反射来导出Schema</h3><p>Spark SQL 的scala接口支持将一个包含类类型(case class)的RDD转换成DataFrame的接口。类类型(case class)定义了表的schema。通过反射机制来读取类类型的参数，其将成为列名(column)，类类型(case class)也支持嵌套以及复杂的数据类型，像Sequence 和 Array。这种RDD可以被隐式地转换成DataFrame并转换成表，然后就可以执行SQL操作。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// sc is an existing SparkContext.</span></span><br><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">sqlContext</span> =</span> <span class="keyword">new</span> org.apache.spark.sql.<span class="type">SQLContext</span>(sc)</span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">// Define the schema using a case class.</span></span><br><span class="line"><span class="comment">// Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,</span></span><br><span class="line"><span class="comment">// you can use custom classes that implement the Product interface.</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(</span>name: <span class="type">String</span>, age: <span class="type">Int</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create an RDD of Person objects and register it as a table.</span></span><br><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">people</span> =</span> sc.textFile(<span class="string">"examples/src/main/resources/people.txt"</span>).map(_.split(<span class="string">","</span>)).map(p =&gt; <span class="type">Person</span>(p(<span class="number">0</span>), p(<span class="number">1</span>).trim.toInt)).toDF()</span><br><span class="line">people.registerTempTable(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL statements can be run by using the sql methods provided by sqlContext.</span></span><br><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">teenagers</span> =</span> sqlContext.sql(<span class="string">"SELECT name, age FROM people WHERE age &gt;= 13 AND age &lt;= 19"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The results of SQL queries are DataFrames and support all the normal RDD operations.</span></span><br><span class="line"><span class="comment">// The columns of a row in the result can be accessed by field index:</span></span><br><span class="line">teenagers.map(t =&gt; <span class="string">"Name: "</span> + t(<span class="number">0</span>)).collect().foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// or by field name:</span></span><br><span class="line">teenagers.map(t =&gt; <span class="string">"Name: "</span> + t.getAs[<span class="type">String</span>](<span class="string">"name"</span>)).collect().foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]</span></span><br><span class="line">teenagers.map(_.getValuesMap[<span class="type">Any</span>](<span class="type">List</span>(<span class="string">"name"</span>, <span class="string">"age"</span>))).collect().foreach(println)</span><br><span class="line"><span class="comment">// Map("name" -&gt; "Justin", "age" -&gt; 19)</span></span><br></pre></td></tr></table></figure></p>
<h3 id="3-2_利用编程接口来得出Schema">3.2 利用编程接口来得出Schema</h3><p>当一个case class类没有事先定义的时候（比如说，一个记录的结构被编码进了一个字符串中，或者是一个文本集，它针对不同的用户可能解析不同的field），则一个DataFrame可以通过下面三步构造成功：</p>
<ol>
<li>从一个存在RDD中构造行RDD（RDD of Rows）；</li>
<li>从步骤1中每行对应的结构类型中构造Schema；</li>
<li>通过CreateDataFrame方法将每行的Schema(schema of rows)应用到行RDD中；</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// sc is an existing SparkContext.</span></span><br><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">sqlContext</span> =</span> <span class="keyword">new</span> org.apache.spark.sql.<span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create an RDD</span></span><br><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">people</span> =</span> sc.textFile(<span class="string">"examples/src/main/resources/people.txt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The schema is encoded in a string</span></span><br><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">schemaString</span> =</span> <span class="string">"name age"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Import Row.</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Import Spark SQL data types</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">StructType</span>,<span class="type">StructField</span>,<span class="type">StringType</span>&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Generate the schema based on the string of schema</span></span><br><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">schema</span> =</span></span><br><span class="line">  <span class="type">StructType</span>(</span><br><span class="line">    schemaString.split(<span class="string">" "</span>).map(fieldName =&gt; <span class="type">StructField</span>(fieldName, <span class="type">StringType</span>, <span class="literal">true</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// Convert records of the RDD (people) to Rows.</span></span><br><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">rowRDD</span> =</span> people.map(_.split(<span class="string">","</span>)).map(p =&gt; <span class="type">Row</span>(p(<span class="number">0</span>), p(<span class="number">1</span>).trim))</span><br><span class="line"></span><br><span class="line"><span class="comment">// Apply the schema to the RDD.</span></span><br><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">peopleDataFrame</span> =</span> sqlContext.createDataFrame(rowRDD, schema)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Register the DataFrames as a table.</span></span><br><span class="line">peopleDataFrame.registerTempTable(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL statements can be run by using the sql methods provided by sqlContext.</span></span><br><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">results</span> =</span> sqlContext.sql(<span class="string">"SELECT name FROM people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The results of SQL queries are DataFrames and support all the normal RDD operations.</span></span><br><span class="line"><span class="comment">// The columns of a row in the result can be accessed by field index or by field name.</span></span><br><span class="line">results.map(t =&gt; <span class="string">"Name: "</span> + t(<span class="number">0</span>)).collect().foreach(println)</span><br></pre></td></tr></table></figure>
<h2 id="4-_支持MySQL等关系型数据库">4. 支持MySQL等关系型数据库</h2><p>对于spark sql支持mysql数据库，只需要将mysql-connector-java-x.x.x-bin.jar加入到spark的系统路径中。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> jdbcDF = sqlContext.read.format(<span class="string">"jdbc"</span>).options(<span class="type">Map</span>(<span class="string">"url"</span> -&gt; <span class="string">"jdbc:mysql://210.45.64.92:3306/weibo_prediction?user=root&amp;password=123456"</span>,<span class="string">"dbtable"</span> -&gt; <span class="string">"weibo_train"</span> ).load();</span><br><span class="line">jdbcDF.registerTempTable(<span class="string">"weibo_train"</span>);</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>这一章的引用：<br>1.<a href="http://stackoverflow.com/questions/29196457/how-to-use-spark-dataframe-with-mysql" target="_blank" rel="external">stackoverflow</a><br>2.<a href="http://www.infoobjects.com/spark-dataframes-and-jdbc/" target="_blank" rel="external">Spark: DataFrame and JDBC</a> </p>
</blockquote>
<h2 id="5-_支持MangoDB等非关系型数据库">5. 支持MangoDB等非关系型数据库</h2><h3 id="5-1_最原始的方法">5.1 最原始的方法</h3><p>类似RDD，目前spark sql对mongoDB的支持，需要采用利用saprkContext接口将mongodb读取成RDD，然后利用第三节的方法来解释RDD，得到DataFrame。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.examples </span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkContext</span>, <span class="type">SparkConf</span>&#125; </span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span> </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.<span class="type">Configuration</span> </span><br><span class="line"><span class="keyword">import</span> org.bson.<span class="type">BSONObject</span> </span><br><span class="line"><span class="keyword">import</span> com.mongodb.hadoop.&#123;<span class="type">MongoInputFormat</span>, <span class="type">BSONFileInputFormat</span>&#125; </span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql </span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SQLContext</span> </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataMigrator</span> &#123;</span> </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span>(</span>args: <span class="type">Array</span>[<span class="type">String</span>])</span><br><span class="line">    &#123; </span><br><span class="line">        <span class="function"><span class="keyword">val</span> <span class="title">conf</span> =</span> <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"Migration    App"</span>).setMaster(<span class="string">"local"</span>) </span><br><span class="line">        <span class="function"><span class="keyword">val</span> <span class="title">sc</span> =</span> <span class="keyword">new</span> <span class="type">SparkContext</span>(conf) </span><br><span class="line">        <span class="function"><span class="keyword">val</span> <span class="title">sqlContext</span> =</span> <span class="keyword">new</span> <span class="type">SQLContext</span>(sc) </span><br><span class="line"></span><br><span class="line">        <span class="comment">// Import statement to implicitly convert an RDD to a DataFrame </span></span><br><span class="line">        <span class="keyword">import</span> sqlContext.implicits._ </span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">val</span> <span class="title">mongoConfig</span> =</span> <span class="keyword">new</span> <span class="type">Configuration</span>() </span><br><span class="line">        mongoConfig.set(<span class="string">"mongo.input.uri"</span>,   <span class="string">"mongodb://localhost:27017/mongosails4.case"</span>) </span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">val</span> <span class="title">mongoRDD</span> =</span> sc.newAPIHadoopRDD(mongoConfig, classOf[<span class="type">MongoInputFormat</span>], classOf[<span class="type">Object</span>], classOf[<span class="type">BSONObject</span>]);     </span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">val</span> <span class="title">count</span> =</span> countsRDD.count()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// the count value is aprox 100,000 </span></span><br><span class="line">        println(<span class="string">"================ PRINTING ====================="</span>) </span><br><span class="line">        println(s<span class="string">"ROW COUNT IS $count"</span>) </span><br><span class="line">        println(<span class="string">"================ PRINTING ====================="</span>) </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>在得到一个RDD后需要</p>
<ol>
<li><p>为你的RDD 创建一个case class</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Data</span>(</span>x: <span class="type">Int</span>, s: <span class="type">String</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>将rdd中的值映射到case class中的实例</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">dataRDD</span> =</span> mongoRDD.values.map &#123; obj =&gt; <span class="type">Data</span>(obj.get(<span class="string">"x"</span>), obj.get(<span class="string">"s"</span>)) &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>利用RDD data, 通过sqlcontext创建dataframe</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">myDF</span> =</span> sqlContext.createDataFrame(dataRDD)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="5-2_更好的解决方案:Stratio">5.2 更好的解决方案:Stratio</h3><p>目前开源api Stratio可以很好的支持 Mongodb，免去了应用程序定义case class的繁琐过程。<br>例子如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">mcInputBuilder</span> =</span> <span class="type">MongodbConfigBuilder</span>(<span class="type">Map</span>(<span class="type">Host</span> -&gt; <span class="type">List</span>(<span class="string">"localhost:27017"</span>), <span class="type">Database</span> -&gt; <span class="string">"marketdata"</span>, <span class="type">Collection</span> -&gt; <span class="string">"minbars"</span>, <span class="type">SamplingRatio</span> -&gt; <span class="number">1.0</span>, <span class="type">WriteConcern</span> -&gt; <span class="type">MongodbWriteConcern</span>.<span class="type">Normal</span>))</span><br><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">readConfig</span> =</span> mcInputBuilder.build()</span><br><span class="line"></span><br><span class="line"><span class="comment">//HiveContext uses Hive's SQL parser with a superset of features of SQLContext so I used that one</span></span><br><span class="line"><span class="comment">//	See http://spark.apache.org/docs/1.4.0/sql-programming-guide.html#starting-point-sqlcontext for more info</span></span><br><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">sqlContext</span> =</span> <span class="keyword">new</span> <span class="type">HiveContext</span>(sc)			<span class="comment">//sc is already defined as a SparkContext by the shell</span></span><br><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">dfOneMin</span> =</span> sqlContext.fromMongoDB(readConfig) 	<span class="comment">//set up the MongoDB collection to read from as a DataFrame</span></span><br><span class="line">dfOneMin.registerTempTable(<span class="string">"minbars"</span>)			<span class="comment">//make the table minbars available to the SQL expressions later</span></span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>这一章节的相关引用<br>1.<a href="https://github.com/Stratio/spark-mongodb" target="_blank" rel="external">Stratio</a><br>2.<a href="http://stackoverflow.com/questions/31838468/save-mongodb-data-to-parquet-file-format-using-apache-spark" target="_blank" rel="external">Save MongoDB data to parquet file format using Apache Spark</a></p>
</blockquote>
<h2 id="6-_支持HBase">6. 支持HBase</h2></span>
      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/SQL/" rel="tag">#SQL</a>
          
            <a href="/tags/Spark/" rel="tag">#Spark</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2015/10/10/解忧杂货店/" rel="next" title="《解忧杂货店》">
                <i class="fa fa-chevron-left"></i> 《解忧杂货店》
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/01/24/2015总结/" rel="prev" title="2015总结">
                2015总结 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

 </div>

        

        
          <div class="comments" id="comments">
            
          </div>
        
      </div>

      
        
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="/images/default_avatar.jpg" alt="Fu Xi" itemprop="image"/>
          <p class="site-author-name" itemprop="name">Fu Xi</p>
        </div>
        <p class="site-description motion-element" itemprop="description"></p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">14</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">0</span>
              <span class="site-state-item-name">分类</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">8</span>
              <span class="site-state-item-name">标签</span>
              </a>
          </div>

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        <div class="links-of-author motion-element">
          
        </div>

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc-indicator-top post-toc-indicator"></div>
          <div class="post-toc">
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-_什么是Spark_SQL?"><span class="nav-number">1.</span> <span class="nav-text">1. 什么是Spark SQL?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1_Spark_SQL提供的数据抽象:DataFrame"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 Spark SQL提供的数据抽象:DataFrame</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-_支持JSON数据类型"><span class="nav-number">2.</span> <span class="nav-text">2. 支持JSON数据类型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1_开始：SQLContext"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 开始：SQLContext</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2_创建DataFrame"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 创建DataFrame</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3_DataFrame的操作"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 DataFrame的操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4_运行SQL查询语句"><span class="nav-number">2.4.</span> <span class="nav-text">2.4 运行SQL查询语句</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-_解释RDD"><span class="nav-number">3.</span> <span class="nav-text">3. 解释RDD</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1_利用反射来导出Schema"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 利用反射来导出Schema</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2_利用编程接口来得出Schema"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 利用编程接口来得出Schema</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-_支持MySQL等关系型数据库"><span class="nav-number">4.</span> <span class="nav-text">4. 支持MySQL等关系型数据库</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-_支持MangoDB等非关系型数据库"><span class="nav-number">5.</span> <span class="nav-text">5. 支持MangoDB等非关系型数据库</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1_最原始的方法"><span class="nav-number">5.1.</span> <span class="nav-text">5.1 最原始的方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2_更好的解决方案:Stratio"><span class="nav-number">5.2.</span> <span class="nav-text">5.2 更好的解决方案:Stratio</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-_支持HBase"><span class="nav-number">6.</span> <span class="nav-text">6. 支持HBase</span></a></li></ol></div>
            
          </div>
          <div class="post-toc-indicator-bottom post-toc-indicator"></div>
        </section>
      

    </div>
  </aside>


      
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner"> <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="icon-next-heart fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Fu Xi</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


 </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  
  
    
    

  


  
  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.2"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.2"></script>
  

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js?v=0.4.5.2" id="motion.global"></script>




  <script type="text/javascript" src="/js/nav-toggle.js?v=0.4.5.2"></script>
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  
<script type="text/javascript" src="/js/bootstrap.scrollspy.js?v=0.4.5.2" id="bootstrap.scrollspy.custom"></script>


<script type="text/javascript" id="sidebar.toc.highlight">
  $(document).ready(function () {
    var tocSelector = '.post-toc';
    var $tocSelector = $(tocSelector);
    var activeCurrentSelector = '.active-current';

    $tocSelector
      .on('activate.bs.scrollspy', function () {
        var $currentActiveElement = $(tocSelector + ' .active').last();

        removeCurrentActiveClass();
        $currentActiveElement.addClass('active-current');

        $tocSelector[0].scrollTop = $currentActiveElement.position().top;
      })
      .on('clear.bs.scrollspy', function () {
        removeCurrentActiveClass();
      });

    function removeCurrentActiveClass () {
      $(tocSelector + ' ' + activeCurrentSelector)
        .removeClass(activeCurrentSelector.substring(1));
    }

    function processTOC () {
      getTOCMaxHeight();
      toggleTOCOverflowIndicators();
    }

    function getTOCMaxHeight () {
      var height = $('.sidebar').height() -
                   $tocSelector.position().top -
                   $('.post-toc-indicator-bottom').height();

      $tocSelector.css('height', height);

      return height;
    }

    function toggleTOCOverflowIndicators () {
      tocOverflowIndicator(
        '.post-toc-indicator-top',
        $tocSelector.scrollTop() > 0 ? 'show' : 'hide'
      );

      tocOverflowIndicator(
        '.post-toc-indicator-bottom',
        $tocSelector.scrollTop() >= $tocSelector.find('ol').height() - $tocSelector.height() ? 'hide' : 'show'
      )
    }

    $(document).on('sidebar.motion.complete', function () {
      processTOC();
    });

    $('body').scrollspy({ target: tocSelector });
    $(window).on('resize', function () {
      if ( $('.sidebar').hasClass('sidebar-active') ) {
        processTOC();
      }
    });

    onScroll($tocSelector);

    function onScroll (element) {
      element.on('mousewheel DOMMouseScroll', function (event) {
          var oe = event.originalEvent;
          var delta = oe.wheelDelta || -oe.detail;

          this.scrollTop += ( delta < 0 ? 1 : -1 ) * 30;
          event.preventDefault();

          toggleTOCOverflowIndicators();
      });
    }

    function tocOverflowIndicator (indicator, action) {
      var $indicator = $(indicator);
      var opacity = action === 'show' ? 0.4 : 0;
      $indicator.velocity ?
        $indicator.velocity('stop').velocity({
          opacity: opacity
        }, { duration: 100 }) :
        $indicator.stop().animate({
          opacity: opacity
        }, 100);
    }

  });
</script>

<script type="text/javascript" id="sidebar.nav">
  $(document).ready(function () {
    var html = $('html');
    var TAB_ANIMATE_DURATION = 200;
    var hasVelocity = $.isFunction(html.velocity);

    $('.sidebar-nav li').on('click', function () {
      var item = $(this);
      var activeTabClassName = 'sidebar-nav-active';
      var activePanelClassName = 'sidebar-panel-active';
      if (item.hasClass(activeTabClassName)) {
        return;
      }

      var currentTarget = $('.' + activePanelClassName);
      var target = $('.' + item.data('target'));

      hasVelocity ?
        currentTarget.velocity('transition.slideUpOut', TAB_ANIMATE_DURATION, function () {
          target
            .velocity('stop')
            .velocity('transition.slideDownIn', TAB_ANIMATE_DURATION)
            .addClass(activePanelClassName);
        }) :
        currentTarget.animate({ opacity: 0 }, TAB_ANIMATE_DURATION, function () {
          currentTarget.hide();
          target
            .stop()
            .css({'opacity': 0, 'display': 'block'})
            .animate({ opacity: 1 }, TAB_ANIMATE_DURATION, function () {
              currentTarget.removeClass(activePanelClassName);
              target.addClass(activePanelClassName);
            });
        });

      item.siblings().removeClass(activeTabClassName);
      item.addClass(activeTabClassName);
    });

    $('.post-toc a').on('click', function (e) {
      e.preventDefault();
      var targetSelector = escapeSelector(this.getAttribute('href'));
      var offset = $(targetSelector).offset().top;
      hasVelocity ?
        html.velocity('stop').velocity('scroll', {
          offset: offset  + 'px',
          mobileHA: false
        }) :
        $('html, body').stop().animate({
          scrollTop: offset
        }, 500);
    });

    // Expand sidebar on post detail page by default, when post has a toc.
    var $tocContent = $('.post-toc-content');
    if (isDesktop() && CONFIG.sidebar === 'post') {
      if ($tocContent.length > 0 && $tocContent.html().trim().length > 0) {
        displaySidebar();
      }
    }
  });
</script>



  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
      if (isMobile()) {
        FastClick.attach(document.body);
      }

      motionIntegrator.bootstrap();
    });
  </script>

  
  

  
  

  
  <script type="text/javascript" src="/js/lazyload.js"></script>
  <script type="text/javascript">
    $(function () {
      $("#posts").find('img').lazyload({
        placeholder: "/images/loading.gif",
        effect: "fadeIn"
      });
    });
  </script>
</body>
</html>
